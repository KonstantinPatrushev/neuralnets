{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb32bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchmetrics -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e44a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa58fe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb5337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5138f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977c1804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import torch\n",
    "import sklearn\n",
    "import datasets\n",
    "import ipywidgets\n",
    "import numpy as np\n",
    "import torch.nn.functional as f\n",
    "import gensim.downloader as api\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from tqdm import tqdm, trange\n",
    "from torchmetrics import Accuracy\n",
    "from ipywidgets import FloatProgress\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dcf5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(word):\n",
    "    if word in word2idx.keys():\n",
    "        return word2idx[word]\n",
    "    \n",
    "    return word2idx['unk']\n",
    "\n",
    "def collate_fn(batch):\n",
    "    max_len = max(len(row['features']) for row in batch)\n",
    "    input_embeds = torch.empty((len(batch), max_len), dtype=torch.long) # матрица фичей для передачи в сеть\n",
    "    labels = torch.empty(len(batch), dtype=torch.long)\n",
    "\n",
    "    for idx, row in enumerate(batch):\n",
    "        to_pad = max_len - len(row['features'])\n",
    "        input_embeds[idx] = torch.cat((row['features'], torch.zeros(to_pad)))\n",
    "        labels[idx] = row['label'] \n",
    "\n",
    "    return {'features': input_embeds, 'labels': labels}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9165f9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# заморозка градиентов на первых N итерациях (для того, чтобы они не вносили неопределенность в веса)\n",
    "\n",
    "def freeze_embeddings(model, req_grad=False):\n",
    "    embeddings = model.embeddings\n",
    "    for c_p in embeddings.parameters():\n",
    "        c_p.requires_grad = req_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab4e0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(model, criterion, optim, metric, num_epochs, loaders, max_grad_norm=2, num_freeze_iter=1000):\n",
    "    freeze_embeddings(model) # чтобы только на 1 итерации была заморозка\n",
    "    for e in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        num_iter = 0\n",
    "        pbar = loaders['train']\n",
    "\n",
    "        for batch in pbar:\n",
    "            if num_iter > num_freeze_iter:\n",
    "                freeze_embeddings(model, True)\n",
    "            optimizer.zero_grad()\n",
    "            input_embeds = batch['features'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            pred = model(input_embeds)\n",
    "            loss = criterion(pred, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            if max_grad_norm:\n",
    "                torch.nn.utils.clip_grad_norm(model.parameters(), max_grad_norm)\n",
    "            \n",
    "            optimizer.step()\n",
    "            num_iter += 1\n",
    "            input_embeds.to('cpu')\n",
    "            labels.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        valid_loss = 0\n",
    "        valid_acc = 0\n",
    "        num_iter = 0\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in loaders['test']:\n",
    "                input_embeds = batch['features'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                pred = model(input_embeds)\n",
    "\n",
    "                valid_loss += criterion(pred, labels)\n",
    "                valid_acc += metric(pred, labels)\n",
    "                num_iter += 1\n",
    "        \n",
    "        print(f'Valid Loss: {valid_loss / num_iter}, Accuracy: {valid_acc/num_iter}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67fc63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# использование предобученных эмбеддингов\n",
    "# (перед этим передать в model нужную архитектуру, обучить, вызвать этот блок и еще раз обучить)\n",
    "with torch.no_grad():\n",
    "    for word, idx in word2idx.items():\n",
    "        if word in word2vec:\n",
    "            model.embedding.weight[idx] = torch.from_numpy(word2vec.get_vector(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9152665",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0xDEAD\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.random.manual_seed(SEED)\n",
    "torch.cuda.random.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e308f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ee4a38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset('ag_news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024d4615",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.WordPunctTokenizer()\n",
    "max_length = 128\n",
    "\n",
    "dataset = dataset.map(lambda x: {\n",
    "    'tokenized': tokenizer.tokenize(x['text'])[:max_length]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd169bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = api.load('glove-twitter-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d23627c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(loaders['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f3b738",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {word: ind for ind, word in enumerate(word2vec.index_to_key)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b324d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x:{\n",
    "    'features': [encode(word) for word in x['tokenized']]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71144b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns(['text', 'tokenized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baaa802",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_format(type='torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d227965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = {k: DataLoader(ds, shuffle=(k=='train'), batch_size=32, collate_fn = collate_fn)\n",
    "for k, ds in dataset.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094dc461",
   "metadata": {},
   "source": [
    "# Сверточная нейросеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b26329",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Model(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, num_classes=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(len(word2idx), embed_size) # инициализация эмбеддингов для всех слов из словаря\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(embed_size, hidden_size, kernel_size=3, padding=1, stride=2),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1, stride=2),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1, stride=2),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.cnn(x)\n",
    "        pred = self.cls(x)\n",
    "\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec6b8ea",
   "metadata": {},
   "source": [
    "## Инициализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd121e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn = CNN_Model(word2vec.vector_size, 50).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_cnn.parameters(), lr=1e-2)\n",
    "metric_cnn = Accuracy('multiclass', num_classes=4).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f5c002",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77fd266",
   "metadata": {},
   "source": [
    "# Классическая рекуррентная нейросеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf65538",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_block(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.W = nn.Parameter(torch.rand(embed_size, hidden_size))\n",
    "        self.U = nn.Parameter(torch.rand(hidden_size, hidden_size))\n",
    "        self.V = nn.Parameter(torch.rand(hidden_size, hidden_size))\n",
    "        self.b_x = nn.Parameter(torch.rand(1, hidden_size))\n",
    "        self.b_h = nn.Parameter(torch.rand(1, hidden_size))\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        hidden = torch.zeros((x.size(0), self.hidden_size)).to(x.device) # h(t-1) размер батча x размер скрытого состояния\n",
    "        seq_len = x.size(1) # длина max предложения\n",
    "        \n",
    "        if hidden is None:\n",
    "            for cur_idx in range(seq_len): # обновляем hidden по каждому номеру слова каждого предл-я в батче\n",
    "                hidden = torch.tanh(x[:, cur_idx] @ self.W + hidden @ self.U + self.b_h)\n",
    "#         print(hidden.is_cuda, self.V.is_cuda, self.b_x.is_cuda)\n",
    "        res = torch.tanh(hidden @ self.V + self.b_x)\n",
    "        return res \n",
    "            \n",
    "            \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08af01e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Model(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, num_classes=4):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(len(word2idx), embed_size)\n",
    "        self.rnn = RNN_block(embed_size, hidden_size)\n",
    "        self.cls = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        hidden = self.rnn(x)\n",
    "        output = self.cls(hidden)\n",
    "        return output\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d5d2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn = RNN_Model(word2vec.vector_size, 50).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_rnn.parameters(), lr=1e-2)\n",
    "metric_rnn = Accuracy('multiclass', num_classes=4).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d61b1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_network(model_cnn, criterion, optimizer, metric_cnn, 1, loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad27767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0626268",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da0b36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_network(model_rnn, criterion, optimizer, metric, 1, loaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e0294d",
   "metadata": {},
   "source": [
    "# GRU (модификация RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3541f96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.w_rh = nn.Parameter(torch.rand(hidden_size, hidden_size))\n",
    "        self.b_rh = nn.Parameter(torch.rand(1, hidden_size))\n",
    "        self.w_rx = nn.Parameter(torch.rand(embed_size, hidden_size))\n",
    "        self.b_rx = nn.Parameter(torch.rand(1, hidden_size))\n",
    "        \n",
    "        self.w_zh = nn.Parameter(torch.rand(hidden_size, hidden_size))\n",
    "        self.b_zh = nn.Parameter(torch.rand(1, hidden_size))\n",
    "        self.w_zx = nn.Parameter(torch.rand(embed_size, hidden_size))\n",
    "        self.b_zx = nn.Parameter(torch.rand(1, hidden_size))\n",
    "        \n",
    "        self.w_nh = nn.Parameter(torch.rand(hidden_size, hidden_size))\n",
    "        self.b_nh = nn.Parameter(torch.rand(1, hidden_size))\n",
    "        self.w_nx = nn.Parameter(torch.rand(embed_size, hidden_size))\n",
    "        self.b_nx = nn.Parameter(torch.rand(1, hidden_size))\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        \n",
    "        if hidden is None:\n",
    "            hidden = torch.zeros((x.size(0), self.hidden_size)).to(x.device)\n",
    "        \n",
    "        seq_len = x.size(1) \n",
    "        for cur_idx in range(seq_len):\n",
    "            r = torch.sigmoid(x[:, cur_idx] @ self.w_rx + self.b_rx + hidden @ self.w_rh + self.b_rh)\n",
    "            z = torch.sigmoid(x[:, cur_idx] @ self.w_zx + self.b_zx + hidden @ self.w_zh + self.b_zh)\n",
    "            n = torch.tanh(x[:, cur_idx] @ self.w_nx + self.b_nx + r * (hidden @ self.w_nh + self.b_nh))\n",
    "            hidden = (1 - z) * n + z * hidden\n",
    "        \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446c736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_Model(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, num_classes=4):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(len(word2idx), embed_size)\n",
    "        self.gru = GRU(embed_size, hidden_size)\n",
    "        self.cls = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        hidden = self.gru(x)\n",
    "        output = self.cls(hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c58953",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru = GRU_Model(embed_size=word2vec.vector_size, hidden_size=50).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_gru.parameters(), lr=1e-2)\n",
    "metric = Accuracy('multiclass', num_classes=4).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533d376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_network(model_gru, criterion, optimizer, metric, 1, loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ea3f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d27ae46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
